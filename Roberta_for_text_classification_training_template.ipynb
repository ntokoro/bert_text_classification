{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import section\n",
    "import time, datetime, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import(\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    TensorDataset,\n",
    "    random_split\n",
    ")\n",
    "import transformers\n",
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    RobertaForSequenceClassification, # Import the model of your choice\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AdamW\n",
    ")\n",
    "from nltk.metrics import ConfusionMatrix # Looks better than the sklearn CM\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stat(predictions, actual) :\n",
    "    #Flatten predictions array\n",
    "    preds = [np.argmax(subarr) for arr in predictions for subarr in arr]\n",
    "    true_labels_1d = []\n",
    "\n",
    "    #Flatten true_labels array\n",
    "    for arr in actual:\n",
    "        true_labels_1d.extend(arr.tolist())\n",
    "\n",
    "    #Print confusion matrixs and measures\n",
    "    cm = ConfusionMatrix(true_labels_1d, preds)\n",
    "    class_rep = classification_report(true_labels_1d, preds)\n",
    "    print(cm)\n",
    "    print(class_rep)\n",
    "    return(preds, true_labels_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "# Max is 512 if using BERT-based models, higher for longformer (2000+)\n",
    "def toke_and_enc(sentences, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = max_len,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                            truncation = True\n",
    "                       )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_dataset, batch_size=16) :\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset) : \n",
    "    train_dataloader = create_data_loader(train_dataset)\n",
    "\n",
    "    #Change the model name and num_labels depending on the task.\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 3)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    epochs = 4 # Change if needed\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "        total_train_loss = 0\n",
    "   \n",
    "        model.train()\n",
    "        print(\"here\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print('.', end =\"\")\n",
    "            b_input_ids = batch[0]\n",
    "            b_input_mask = batch[1]\n",
    "            b_labels = batch[2]\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,  labels=b_labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_dataset, model) : \n",
    "    \n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=16)\n",
    "   \n",
    "    print('here')\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        print(\".\", end =\" \")\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "        with torch.no_grad():\n",
    "              outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    print('DONE.')\n",
    "    return(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  559 training samples\n",
      "  140 test samples\n",
      "Max sentence length:  355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ========\n",
      "here\n",
      "...................................  Average training loss: 1.06\n",
      "======== Epoch 2 / 4 ========\n",
      "here\n",
      "...................................  Average training loss: 0.54\n",
      "======== Epoch 3 / 4 ========\n",
      "here\n",
      "...................................  Average training loss: 0.33\n",
      "======== Epoch 4 / 4 ========\n",
      "here\n",
      "...................................  Average training loss: 0.24\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "pathtofile = './data/data.csv'\n",
    "df = pd.read_csv(pathtofile, encoding='utf-8')\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "dftrain=df.sample(frac=0.8,random_state=seed_val)\n",
    "dftest=df.drop(dftrain.index)\n",
    "print('{:>5,} training samples'.format(len(dftrain)))\n",
    "print('{:>5,} test samples'.format(len(dftest)))\n",
    "\n",
    "# set tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base') #Change this if using a different model\n",
    "\n",
    "max_len = 0\n",
    "for text in df['Text'].values:\n",
    "    len_txt = len(tokenizer.encode(text, add_special_tokens=True))\n",
    "    max_len = max(max_len, len_txt)\n",
    "print('Max sentence length: ', max_len)\n",
    "\n",
    "# construct the input for the training phase\n",
    "All_text_train = dftrain['Text'].values # Use appropriate column names\n",
    "age_labels_train = dftrain['Gender'].values\n",
    "Input_ids_train, Attention_masks_train = toke_and_enc(All_text_train, max_len)\n",
    "age_labels_train = torch.tensor(age_labels_train)\n",
    "Train_dataset = TensorDataset(Input_ids_train, Attention_masks_train, age_labels_train)\n",
    "model_train = train_model(Train_dataset)\n",
    "# Save model (optional)\n",
    "# model_train.save_pretrained() # Uncomment to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result\n",
      "here\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . DONE.\n",
      "  |   0   1   2 |\n",
      "--+-------------+\n",
      "0 |<198>  2   2 |\n",
      "1 |   6<193>  2 |\n",
      "2 |  11   1<144>|\n",
      "--+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       202\n",
      "           1       0.98      0.96      0.97       201\n",
      "           2       0.97      0.92      0.95       156\n",
      "\n",
      "    accuracy                           0.96       559\n",
      "   macro avg       0.96      0.95      0.96       559\n",
      "weighted avg       0.96      0.96      0.96       559\n",
      "\n",
      "\n",
      "Testing result\n",
      "here\n",
      ". . . . . . . . . DONE.\n",
      "  |  0  1  2 |\n",
      "--+----------+\n",
      "0 |<43> 3  2 |\n",
      "1 |  4<46> . |\n",
      "2 |  8  2<32>|\n",
      "--+----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.83        48\n",
      "           1       0.90      0.92      0.91        50\n",
      "           2       0.94      0.76      0.84        42\n",
      "\n",
      "    accuracy                           0.86       140\n",
      "   macro avg       0.87      0.86      0.86       140\n",
      "weighted avg       0.87      0.86      0.86       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the model on the training data set to collect stat and output\n",
    "print('Training result')\n",
    "trainpred, trainactual = test_model(Train_dataset, model_train)\n",
    "train_pred, train_actual  = calculate_stat(trainpred, trainactual)\n",
    "dftrain['Mpred'] = train_pred\n",
    "dftrain['Mactual'] = train_actual\n",
    "dftrain.to_csv('outtrain.csv', encoding='utf-8')\n",
    "print()\n",
    "\n",
    "# Run the model on the test data set to collect stat and output\n",
    "print('Testing result')\n",
    "All_text_test = dftest['Text'].values\n",
    "age_labels_test = dftest['Gender'].values\n",
    "Input_ids_test, Attention_masks_test = toke_and_enc(All_text_test, max_len)\n",
    "age_labels_test = torch.tensor(age_labels_test)\n",
    "Test_dataset = TensorDataset(Input_ids_test, Attention_masks_test, age_labels_test)\n",
    "testpred, testactual = test_model(Test_dataset, model_train)\n",
    "test_pred, test_actual  = calculate_stat(testpred, testactual)\n",
    "dftest['Mpred'] = test_pred\n",
    "dftest['Mactual'] = test_actual\n",
    "dftest.to_csv('outtest.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
